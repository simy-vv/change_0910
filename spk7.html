<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>spk</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
        }
        .container {
            display: flex;
            flex-direction: row;
            align-items: flex-start;
            justify-content: center;
        }
        #output, #response {
            width: 300px;
            height: 150px;
            border: 1px solid #000;
            padding: 10px;
            font-size: 1.2rem;
            overflow-y: auto;
            margin-right: 20px;
        }
        .controls {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
            justify-content: center;
            margin-bottom: 20px;
            width: 90%;
        }
        input, button {
            padding: 10px;
            font-size: 1rem;
            margin-right: 10px;
        }
        input {
            width: 200px;
        }
        canvas {
            border: 1px solid black;
            width: 300px;
            height: 150px;
        }
    </style>
</head>
<body>
    <h1>2024年11月A高２年２組実習事後学習</h1>
    
    <!-- APIキー入力エリア -->
    <div class="controls">
        <input type="password" id="apiKeyInput" placeholder="認証">
    </div>
    
    <!-- 閾値入力エリア -->
    <div class="controls">
        <label for="thresholdLow">下限: </label>
        <input type="number" id="thresholdLow" value="30">
        <label for="thresholdHigh">上限: </label>
        <input type="number" id="thresholdHigh" value="80">
    </div>
    
    <!-- トークン数指定エリア -->
    <div class="controls">
        <label for="maxTokens">文字数: </label>
        <input type="number" id="maxTokens" value="100">
    </div>

    <!-- 音声認識結果とChatGPTレスポンス表示エリア -->
    <div class="container">
        <div id="output"></div>
        <div id="response"></div>
    </div>
    
    <!-- 音量波形表示エリア -->
    <canvas id="volumeGraph" width="300" height="150"></canvas>
    
    <!-- コントロールエリア -->
    <div class="controls">
        <button id="startBtn">スタート</button>
        <button id="stopBtn">ストップ</button>
        <button id="playVoiceBtn" disabled>音声再生</button> <!-- 音声再生ボタン -->
    </div>

    <script>
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const playVoiceBtn = document.getElementById('playVoiceBtn'); // 音声再生ボタン
        const outputDiv = document.getElementById('output');
        const responseDiv = document.getElementById('response');
        const apiKeyInput = document.getElementById('apiKeyInput');
        const thresholdLowInput = document.getElementById('thresholdLow');
        const thresholdHighInput = document.getElementById('thresholdHigh');
        const maxTokensInput = document.getElementById('maxTokens'); // トークン数の入力フィールドを取得
        const canvas = document.getElementById('volumeGraph');
        const ctx = canvas.getContext('2d');
        let recognition;
        let audioContext;
        let analyser;
        let microphone;
        let javascriptNode;
        let dataArray = [];
        let currentDecibels = 0; // 現在の音量レベル
        let aiSpeaking = false; // AIが発話中かどうかを判別
        let chatResponse = ''; // 音声再生用に保持

        // Web Speech APIのサポート確認
        if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            recognition = new SpeechRecognition();
            recognition.lang = 'ja-JP';  // 言語を日本語に設定
            recognition.interimResults = false;  // 中間結果でも送信するように設定
            recognition.continuous = true;  // 継続的に認識する

            recognition.onresult = (event) => {
                let transcript = '';
                for (let i = event.resultIndex; i < event.results.length; i++) {
                    transcript += event.results[i][0].transcript;
                }
                outputDiv.textContent = transcript;

                // AIの発言中はクエリを送信しない
                if (aiSpeaking) {
                    console.log("AIの発話中のため、クエリは送信されません。");
                    return;
                }

                // 音量が閾値の範囲内か確認
                const thresholdLow = parseInt(thresholdLowInput.value, 10);
                const thresholdHigh = parseInt(thresholdHighInput.value, 10);

                if (currentDecibels >= thresholdLow && currentDecibels <= thresholdHigh) {
                    // APIキーを取得
                    const apiKey = apiKeyInput.value;
                    if (!apiKey) {
                        alert('APIキー');
                        return;
                    }

                    // トークン数を取得
                    const maxTokens = parseInt(maxTokensInput.value, 10);

                    // ChatGPT APIに送信
                    fetch('https://api.openai.com/v1/chat/completions', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': `Bearer ${apiKey}`
                        },
                        body: JSON.stringify({
                            model: 'gpt-4',
                            messages: [
                                { role: 'user', content: transcript },
                                { role: 'system', content: `日本語で回答してください。クエリの文法が正しくないときは正しい文法に直してuserに返してください。${maxTokensInput.value}以内の文字数で回答します`  }
                            ],
                            max_tokens: maxTokens // トークン数を使用
                        })
                    })
                    .then(response => {
                        if (!response.ok) {
                            throw new Error(`HTTP error! status: ${response.status}`);
                        }
                        return response.json();
                    })
                    .then(data => {
                        chatResponse = data.choices[0].message.content;
                        responseDiv.textContent = chatResponse;

                        // 音声再生ボタンを有効にする
                        playVoiceBtn.disabled = false;
                    })
                    .catch(error => {
                        responseDiv.textContent = `Error: ${error.message}`;
                        console.error('Error:', error);
                    });
                } else {
                    console.log('音量が閾値範囲外のため、クエリは送信されませんでした。');
                }
            };

            recognition.onerror = (event) => {
                if (event.error === 'no-speech') {
                    console.warn("音声が検出されませんでした。再度話しかけてください。");
                    recognition.start(); // no-speechエラーが発生した場合、音声認識を再開
                } else {
                    console.error('エラーが発生しました: ', event.error);
                    responseDiv.textContent = `認識エラー: ${event.error}`;
                }
            };

            recognition.onend = () => {
                if (!aiSpeaking) {
                    recognition.start(); // 音声認識が停止した場合でも、AIが話していない場合は再開
                }
                console.log('音声認識が終了しました');
            };

            startBtn.addEventListener('click', () => {
                navigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {
                    recognition.start();
                    startVolumeMeter(stream);  // 音量計測を開始
                    console.log('音声認識と音量計測が開始されました');
                               }).catch((error) => {
                    console.error('マイクアクセスエラー: ', error);
                    alert('マイクへのアクセスが拒否されました');
                });
            });

            stopBtn.addEventListener('click', () => {
                recognition.stop();
                if (audioContext) {
                    audioContext.close();  // 音量計測を停止
                }
                console.log('音声認識と音量計測が停止されました');
            });

            // 音声再生ボタンのイベントリスナー
            playVoiceBtn.addEventListener('click', () => {
                if (chatResponse) {
                    const utterance = new SpeechSynthesisUtterance(chatResponse);
                    utterance.onend = () => {
                        aiSpeaking = false;  // 発話終了後にAIの発話フラグをリセット
                        recognition.start();  // 音声認識を再開
                    };
                    window.speechSynthesis.speak(utterance);
                    aiSpeaking = true;  // 発話開始時にAIの発話フラグをセット
                    playVoiceBtn.disabled = true;  // 音声再生後、ボタンを無効化
                }
            });

            // 音量計測の開始
            function startVolumeMeter(stream) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                microphone = audioContext.createMediaStreamSource(stream);
                javascriptNode = audioContext.createScriptProcessor(2048, 1, 1);

                analyser.smoothingTimeConstant = 0.8;
                analyser.fftSize = 1024;

                microphone.connect(analyser);
                analyser.connect(javascriptNode);
                javascriptNode.connect(audioContext.destination);

                javascriptNode.onaudioprocess = () => {
                    const array = new Uint8Array(analyser.frequencyBinCount);
                    analyser.getByteFrequencyData(array);

                    let values = 0;
                    for (let i = 0; i < array.length; i++) {
                        values += array[i];
                    }

                    const average = values / array.length;
                    currentDecibels = Math.round(average);

                    // 音量データを保存
                    dataArray.push(currentDecibels);
                    if (dataArray.length > canvas.width) {
                        dataArray.shift();  // 古いデータを削除
                    }

                    // 波形と閾値ラインを描画
                    drawVolumeGraph();
                };

                // グラフを定期的に再描画
                function updateGraph() {
                    drawVolumeGraph();  // グラフを描画
                    requestAnimationFrame(updateGraph);  // 次のフレームで再描画
                }

                updateGraph();  // 最初の描画を開始
            }

            // 音量波形と閾値ラインを描画する関数
            function drawVolumeGraph() {
                ctx.clearRect(0, 0, canvas.width, canvas.height);  // キャンバスをクリア

                // 波形を描画
                ctx.beginPath();
                ctx.moveTo(0, canvas.height - dataArray[0]);  // 最初のポイントに移動
                for (let i = 1; i < dataArray.length; i++) {
                    ctx.lineTo(i, canvas.height - dataArray[i]);  // 各ポイントに線を引く
                }
                ctx.strokeStyle = 'blue';  // 波形の色
                ctx.stroke();

                // 閾値ラインを描画
                const thresholdLow = parseInt(thresholdLowInput.value, 10);
                const thresholdHigh = parseInt(thresholdHighInput.value, 10);

                // 下限閾値のラインを描画
                const lowThresholdY = canvas.height - (thresholdLow / 100) * canvas.height;
                ctx.beginPath();
                ctx.moveTo(0, lowThresholdY);
                ctx.lineTo(canvas.width, lowThresholdY);
                ctx.strokeStyle = 'red';  // 下限閾値のラインの色
                ctx.stroke();

                // 上限閾値のラインを描画
                const highThresholdY = canvas.height - (thresholdHigh / 100) * canvas.height;
                ctx.beginPath();
                ctx.moveTo(0, highThresholdY);
                ctx.lineTo(canvas.width, highThresholdY);
                ctx.strokeStyle = 'green';  // 上限閾値のラインの色
                ctx.stroke();
            }

        } else {
            outputDiv.textContent = 'お使いのブラウザはWeb Speech APIに対応していません。';
        }
    </script>
</body>
</html>
